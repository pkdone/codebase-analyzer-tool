Generating Insights From LLM Without Blowing The LLM's Token Context Size. Act as a senior programmer analyze the code in a TypeScript application, where the content of each file in the application's codebase is shown below in a code block. When generating insights using an LLM, the data the application obtaints from all sources files metadata (which were previsouly captured in the database), the size of the generated prompt is exceeding the LLM's token size limit. The logs from running the applicable show this, as shown below. Come up with a strategy for avoiding the need to shorten the prompt (which loses data) and a plan how to address this capability as a feature in the codebase.Provide references to the specific part(s) of that code that should be altered and/or added to, with a plan on how to apply and verify the changes/additions required. Include the list or relative file paths of each source file affected.

```
/usr/bin/node ./dist/src/cli/generate-insights-from-db.js
LLM environment variables loaded and registered.
Project name 'fineract-develop' derived and registered.
LLM model family 'VertexAIGemini' registered.
LLM services registered
LLMProviderManager: Loaded provider for model family 'VertexAIGemini': VertexAI Gemini
LLMProviderManager registered with async initialization
LLMRouter registered as singleton
MongoDB Client Factory initialized and registered as singleton
Connecting MongoDB client to: mongodb+srv://REDACTED:REDACTED@democluster.s703u.mongodb.net/
MongoDB Client connected and registered as instance
Repositories registered
Capture components registered
Insights components registered
Reporting components registered
API components registered
Querying components registered
LLM-dependent capture components registered
LLM-dependent insights components registered
LLM-dependent querying components registered
Internal helper components registered
LLM-dependent tasks registered with simplified singleton registrations
Main executable tasks registered
START: 2025-10-08T09:06:00.403Z
Router LLMs to be used: VertexAIGemini (embeddings: gemini-embedding-001, completions - primary: gemini-2.5-pro, secondary: gemini-2.5-flash)
Generating insights for project: fineract-develop
LLM invocation event types that will be recorded:
╭┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┬┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┬┄┄┄┄┄┄┄┄╮
┊                ┊ description                                                             ┊ symbol ┊
├┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┼┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┼┄┄┄┄┄┄┄┄┤
┊ SUCCESS        ┊ LLM invocation succeeded                                                ┊ >      ┊
┊ FAILURE        ┊ LLM invocation failed (no data produced)                                ┊ !      ┊
┊ SWITCH         ┊ Switched to secondary LLM to try to process request                     ┊ +      ┊
┊ OVERLOAD_RETRY ┊ Retried calling LLM due to provider overload or network issue           ┊ ?      ┊
┊ HOPEFUL_RETRY  ┊ Retried calling LLM due to invalid JSON response (a hopeful re-attempt) ┊ ~      ┊
┊ CROP           ┊ Cropping prompt due to excessive size, before resending                 ┊ -      ┊
╰┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┴┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┄┴┄┄┄┄┄┄┄┄╯: {SUCCESS: {…}, FAILURE: {…}, SWITCH: {…}, OVERLOAD_RETRY: {…}, HOPEFUL_RETRY: {…}, CROP: {…}}
┌────────────────┬───────────────────────────────────────────────────────────────────────────┬────────┐
│ (index)        │ description                                                               │ symbol │
├────────────────┼───────────────────────────────────────────────────────────────────────────┼────────┤
│ SUCCESS        │ 'LLM invocation succeeded'                                                │ '>'    │
│ FAILURE        │ 'LLM invocation failed (no data produced)'                                │ '!'    │
│ SWITCH         │ 'Switched to secondary LLM to try to process request'                     │ '+'    │
│ OVERLOAD_RETRY │ 'Retried calling LLM due to provider overload or network issue'           │ '?'    │
│ HOPEFUL_RETRY  │ 'Retried calling LLM due to invalid JSON response (a hopeful re-attempt)' │ '~'    │
│ CROP           │ 'Cropping prompt due to excessive size, before resending'                 │ '-'    │
└────────────────┴───────────────────────────────────────────────────────────────────────────┴────────┘
Codebase chars length: 36994913, Estimated prompt tokens: 10276364
Processing Application Description
Processing Technologies
Processing Business Processes
Processing Bounded Contexts
Processing Aggregates
Processing Entities
Processing Repositories
Processing Potential Microservices
LLM prompt tokens used 1287597 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: boundedContexts
  * purpose: completions
  * modelQuality: primary
  * outputFormat: json
+
LLM prompt tokens used 1288095 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: businessProcesses
  * purpose: completions
  * modelQuality: primary
  * outputFormat: json
+
LLM prompt tokens used 1287219 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: appDescription
  * purpose: completions
  * modelQuality: primary
  * outputFormat: json
+
LLM prompt tokens used 1289638 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: potentialMicroservices
  * purpose: completions
  * modelQuality: primary
  * outputFormat: json
+
LLM prompt tokens used 1287777 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: repositories
  * purpose: completions
  * modelQuality: primary
  * outputFormat: json
+
LLM prompt tokens used 1287770 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: entities
  * purpose: completions
  * modelQuality: primary
  * outputFormat: json
+
LLM prompt tokens used 1287551 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: technologies
  * purpose: completions
  * modelQuality: primary
  * outputFormat: json
+
LLM prompt tokens used 1288007 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: aggregates
  * purpose: completions
  * modelQuality: primary
  * outputFormat: json
+
LLM prompt tokens used 1287777 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: repositories
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1287597 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: boundedContexts
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1287219 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: appDescription
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1289638 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: potentialMicroservices
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1288095 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: businessProcesses
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1288007 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: aggregates
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1287770 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: entities
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1287551 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: technologies
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1050367 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: repositories
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1050314 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: appDescription
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1050338 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: technologies
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1050484 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: potentialMicroservices
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1050341 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: boundedContexts
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1050377 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: businessProcesses
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1050383 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: aggregates
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 1050370 plus completion tokens used 0 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: entities
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
>
Captured main Application Description summary details into database
LLM prompt tokens used 893732 plus completion tokens used 311 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: repositories
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
>
Captured main Bounded Contexts summary details into database
LLM prompt tokens used 893734 plus completion tokens used 314 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: entities
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 893717 plus completion tokens used 316 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: aggregates
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 893549 plus completion tokens used 319 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: potentialMicroservices
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 893744 plus completion tokens used 315 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: technologies
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 893679 plus completion tokens used 2764 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: businessProcesses
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 760866 plus completion tokens used 313 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: repositories
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 760862 plus completion tokens used 315 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: entities
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 760792 plus completion tokens used 310 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: potentialMicroservices
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 760842 plus completion tokens used 316 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: businessProcesses
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 760843 plus completion tokens used 315 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: technologies
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 760870 plus completion tokens used 316 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: aggregates
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 648106 plus completion tokens used 312 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: repositories
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 648110 plus completion tokens used 316 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: entities
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 648096 plus completion tokens used 318 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: potentialMicroservices
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 648086 plus completion tokens used 317 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: technologies
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 648088 plus completion tokens used 2216 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: businessProcesses
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 648122 plus completion tokens used 317 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: aggregates
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
LLM prompt tokens used 550921 plus completion tokens used 316 exceeded EITHER: 1) the model's total token limit of 1048576, or: 2) the model's completion tokens limit
  * resource: repositories
  * purpose: completions
  * modelQuality: secondary
  * outputFormat: json
-
```
