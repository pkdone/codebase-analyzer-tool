import { injectable, inject } from "tsyringe";
import { z } from "zod";
import LLMRouter from "../../../../common/llm/llm-router";
import type { AppSummariesRepository } from "../../../repositories/app-summaries/app-summaries.repository.interface";
import { repositoryTokens } from "../../../di/tokens";
import { llmTokens } from "../../../di/tokens";
import { coreTokens } from "../../../di/tokens";
import type { IInsightsProcessor } from "../insights-processor.interface";
import {
  formatDirectoryAsMarkdown,
  type DirectoryFormattingConfig,
} from "../../../../common/utils/directory-to-markdown";
import { fileProcessingConfig } from "../../../components/capture/config/file-processing.config";
import type { EnvVars } from "../../../env/env.types";
import { logOneLineWarning } from "../../../../common/utils/logging";
import { renderPrompt } from "../../../prompts/prompt-renderer";
import { LLMOutputFormat } from "../../../../common/llm/types/llm.types";
import { appSummaryPromptMetadata as summaryCategoriesConfig } from "../../../prompts/definitions/app-summaries";
import { BASE_PROMPT_TEMPLATE } from "../../../prompts/templates";
import { appSummaryRecordCategoriesSchema } from "../insights.types";

// Type for validating the LLM response for all categories
type AppSummaryRecordCategories = z.infer<typeof appSummaryRecordCategoriesSchema>;

// VertexAI has compatibility issues with complex JSON schemas containing $refs generated by zod
// This flag indicates that the all-categories schema contains complex $refs that are incompatible with VertexAI
const ALL_CATEGORIES_SCHEMA_IS_VERTEXAI_INCOMPATIBLE = true;

/**
 * Class to generate insights from raw code
 */
@injectable()
export default class InsightsFromRawCodeGenerator implements IInsightsProcessor {
  // Private fields
  private readonly llmProviderDescription: string;

  /**
   * Creates a new InsightsFromRawCodeGenerator.
   */
  constructor(
    @inject(repositoryTokens.AppSummariesRepository)
    private readonly appSummariesRepository: AppSummariesRepository,
    @inject(llmTokens.LLMRouter) private readonly llmRouter: LLMRouter,
    @inject(coreTokens.ProjectName) private readonly projectName: string,
    @inject(coreTokens.EnvVars) private readonly env: EnvVars,
  ) {
    this.llmProviderDescription = this.llmRouter.getModelsUsedDescription();
  }

  /**
   * Generate insights from raw code and store in the database
   */
  async generateAndStoreInsights(): Promise<void> {
    const config: DirectoryFormattingConfig = {
      folderIgnoreList: fileProcessingConfig.FOLDER_IGNORE_LIST,
      filenameIgnorePrefix: fileProcessingConfig.FILENAME_PREFIX_IGNORE,
      binaryFileExtensionIgnoreList: fileProcessingConfig.BINARY_FILE_EXTENSION_IGNORE_LIST,
    };
    const codeBlocksContent = await formatDirectoryAsMarkdown(this.env.CODEBASE_DIR_PATH, config);
    await this.generateDataForAllCategories(codeBlocksContent);
  }

  /**
   * Generate insights from raw code and store in the database
   */
  private async generateDataForAllCategories(codeBlocksContent: string): Promise<void> {
    try {
      console.log(`Processing all categories in one go`);
      const allCategoriesSummaryData =
        await this.getAllCategoriesSummaryAsValidatedJSON(codeBlocksContent);
      if (!allCategoriesSummaryData) return;
      await this.appSummariesRepository.createOrReplaceAppSummary({
        projectName: this.projectName,
        llmProvider: this.llmProviderDescription,
        ...allCategoriesSummaryData,
      });
      console.log(`Captured summary details of all categories into database`);
    } catch (error: unknown) {
      logOneLineWarning(
        `Unable to generate summary data for all app categories details into database`,
        error,
      );
    }
  }

  /**
   * Generate insights as validated JSON by calling the LLM with a prompt covering all categories
   */
  private async getAllCategoriesSummaryAsValidatedJSON(
    codeBlocksContent: string,
  ): Promise<AppSummaryRecordCategories | null> {
    try {
      const instructions: readonly string[] = Object.values(summaryCategoriesConfig).flatMap(
        (category) => {
          const inst = category.instructions;
          return inst;
        },
      );
      const prompt = this.createInsightsAllCategoriesPrompt(instructions, codeBlocksContent);
      const llmResponse = await this.llmRouter.executeCompletion("all-categories", prompt, {
        outputFormat: LLMOutputFormat.JSON,
        jsonSchema: appSummaryRecordCategoriesSchema,
        hasComplexSchema: ALL_CATEGORIES_SCHEMA_IS_VERTEXAI_INCOMPATIBLE,
      });
      return llmResponse;
    } catch (error: unknown) {
      logOneLineWarning(
        `${error instanceof Error ? error.message : "Unknown error"} for getting summary data for all categories`,
      );
      return null;
    }
  }

  /**
   * Create a prompt for the LLM to generate insights for all categories
   */
  private createInsightsAllCategoriesPrompt(
    instructions: readonly string[],
    codeBlocksContent: string,
  ): string {
    const allCategoriesConfig = {
      introTextTemplate:
        "Act as a senior developer analyzing the code in a legacy application. Based on the list of file summaries shown below in the section marked '{{dataBlockHeader}}', return a JSON response that contains {{instructionsText}}.",
      instructions,
      responseSchema: appSummaryRecordCategoriesSchema,
      template: BASE_PROMPT_TEMPLATE,
      dataBlockHeader: "FILE_SUMMARIES" as const,
      wrapInCodeBlock: false,
    };
    return renderPrompt(allCategoriesConfig, { content: codeBlocksContent });
  }
}
