import { injectable, inject } from "tsyringe";
import { z } from "zod";
import LLMRouter from "../../llm/core/llm-router";
import type { AppSummaryRepository } from "../../repositories/app-summary/app-summaries.repository.interface";
import { TOKENS } from "../../tokens";
import type { ApplicationInsightsProcessor } from "./insights-generator.interface";
import { formatCodebaseForPrompt } from "./utils/codebase-formatter";
import type { EnvVars } from "../../env/env.types";
import { logErrorMsgAndDetail, logWarningMsg } from "../../common/utils/logging";
import { createPromptFromConfig } from "../../llm/utils/prompt-templator";
import { LLMOutputFormat } from "../../llm/types/llm.types";
import { summaryCategoriesConfig } from "./insights-generation.config";
import { appSummaryRecordCategoriesSchema } from "./insights.types";

// Type for validating the LLM response for all categories
type AppSummaryRecordCategories = Partial<z.infer<typeof appSummaryRecordCategoriesSchema>>;

// VertexAI has compatibility issues with complex JSON schemas containing $refs generated by zod
// This flag indicates that the all-categories schema contains complex $refs that are incompatible with VertexAI
const ALL_CATEGORIES_SCHEMA_IS_VERTEXAI_INCOMPATIBLE = true;

/**
 * Class to generate insights from raw code
 */
@injectable()
export default class InsightsFromRawCodeGenerator implements ApplicationInsightsProcessor {
  // Private fields
  private readonly APP_CATEGORIES_SUMMARIZER_TEMPLATE =
    "Act as a senior developer analyzing the code in a legacy application. Analyze the application's codebase shown below in the section marked 'SOURCES', and based on the code, return a JSON response that contains:\n\n {{specificInstructions}}.\n\nThe JSON response must follow this JSON schema:\n```json\n{{jsonSchema}}\n```\n\n{{forceJSON}}\n\nSOURCES:\n{{codeContent}}";
  private readonly llmProviderDescription: string;

  /**
   * Creates a new InsightsFromRawCodeGenerator.
   */
  constructor(
    @inject(TOKENS.AppSummaryRepository)
    private readonly appSummariesRepository: AppSummaryRepository,
    @inject(TOKENS.LLMRouter) private readonly llmRouter: LLMRouter,
    @inject(TOKENS.ProjectName) private readonly projectName: string,
    @inject(TOKENS.EnvVars) private readonly env: EnvVars,
  ) {
    this.llmProviderDescription = this.llmRouter.getModelsUsedDescription();
  }

  /**
   * Generate insights from raw code and store in the database
   */
  async generateAndStoreInsights(): Promise<void> {
    const codeBlocksContent = await formatCodebaseForPrompt(this.env.CODEBASE_DIR_PATH);
    await this.generateDataForAllCategories(codeBlocksContent);
  }

  /**
   * Generate insights from raw code and store in the database
   */
  private async generateDataForAllCategories(codeBlocksContent: string): Promise<void> {
    try {
      console.log(`Processing all categories in one go`);
      const allCategoriesSummaryData =
        await this.getAllCategoriesSummaryAsValidatedJSON(codeBlocksContent);
      if (!allCategoriesSummaryData) return;
      await this.appSummariesRepository.createOrReplaceAppSummary({
        projectName: this.projectName,
        llmProvider: this.llmProviderDescription,
        ...allCategoriesSummaryData,
      });
      console.log(`Captured summary details of all categories into database`);
    } catch (error: unknown) {
      logErrorMsgAndDetail(
        `Unable to generate summary data for all app categories details into database`,
        error,
      );
    }
  }

  /**
   * Generate insights as validated JSON by calling the LLM with a prompt covering all categories
   */
  private async getAllCategoriesSummaryAsValidatedJSON(
    codeBlocksContent: string,
  ): Promise<AppSummaryRecordCategories | null> {
    try {
      const instructions = Object.values(summaryCategoriesConfig)
        .map((category) => `* ${category.description}`)
        .join("\n"); // Concatenate category descriptions. prefixed with "* " followed by newline
      const prompt = this.createInsightsAllCategoriesPrompt(instructions, codeBlocksContent);
      const llmResponse = await this.llmRouter.executeCompletion<AppSummaryRecordCategories>(
        "all-categories",
        prompt,
        {
          outputFormat: LLMOutputFormat.JSON,
          jsonSchema: appSummaryRecordCategoriesSchema,
          hasComplexSchema: ALL_CATEGORIES_SCHEMA_IS_VERTEXAI_INCOMPATIBLE,
        },
      );
      return llmResponse;
    } catch (error: unknown) {
      logWarningMsg(
        `${error instanceof Error ? error.message : "Unknown error"} for getting summary data for all categories`,
      );
      return null;
    }
  }

  /**
   * Create a prompt for the LLM to generate insights for all categories
   */
  private createInsightsAllCategoriesPrompt(
    instructions: string,
    codeBlocksContent: string,
  ): string {
    return createPromptFromConfig(
      this.APP_CATEGORIES_SUMMARIZER_TEMPLATE,
      "codebase codeblock",
      instructions,
      appSummaryRecordCategoriesSchema,
      codeBlocksContent,
    );
  }
}
