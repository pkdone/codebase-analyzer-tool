import { injectable, inject } from "tsyringe";
import { z } from "zod";
import LLMRouter from "../../llm/core/llm-router";
import type { AppSummariesRepository } from "../../repositories/app-summaries/app-summaries.repository.interface";
import { repositoryTokens } from "../../di/repositories.tokens";
import { llmTokens } from "../../llm/core/llm.tokens";
import { coreTokens } from "../../di/core.tokens";
import type { ApplicationInsightsProcessor } from "./insights.types";
import { formatCodebaseForPrompt } from "./utils/codebase-formatter";
import type { EnvVars } from "../../env/env.types";
import { logErrorMsgAndDetail, logWarningMsg } from "../../common/utils/logging";
import { Prompt } from "../../prompts/prompt";
import { LLMOutputFormat } from "../../llm/types/llm.types";
import { appSummaryPromptMetadata as summaryCategoriesConfig } from "../../prompts/templates/app-summaries.prompts";
import { SINGLE_PASS_INSIGHTS_TEMPLATE } from "../../prompts/templates/app-summaries-strategy.prompts";
import { appSummaryRecordCategoriesSchema } from "./insights.types";

// Type for validating the LLM response for all categories
type AppSummaryRecordCategories = Partial<z.infer<typeof appSummaryRecordCategoriesSchema>>;

// VertexAI has compatibility issues with complex JSON schemas containing $refs generated by zod
// This flag indicates that the all-categories schema contains complex $refs that are incompatible with VertexAI
const ALL_CATEGORIES_SCHEMA_IS_VERTEXAI_INCOMPATIBLE = true;

/**
 * Class to generate insights from raw code
 */
@injectable()
export default class InsightsFromRawCodeGenerator implements ApplicationInsightsProcessor {
  // Private fields
  private readonly llmProviderDescription: string;

  /**
   * Creates a new InsightsFromRawCodeGenerator.
   */
  constructor(
    @inject(repositoryTokens.AppSummariesRepository)
    private readonly appSummariesRepository: AppSummariesRepository,
    @inject(llmTokens.LLMRouter) private readonly llmRouter: LLMRouter,
    @inject(coreTokens.ProjectName) private readonly projectName: string,
    @inject(coreTokens.EnvVars) private readonly env: EnvVars,
  ) {
    this.llmProviderDescription = this.llmRouter.getModelsUsedDescription();
  }

  /**
   * Generate insights from raw code and store in the database
   */
  async generateAndStoreInsights(): Promise<void> {
    const codeBlocksContent = await formatCodebaseForPrompt(this.env.CODEBASE_DIR_PATH);
    await this.generateDataForAllCategories(codeBlocksContent);
  }

  /**
   * Generate insights from raw code and store in the database
   */
  private async generateDataForAllCategories(codeBlocksContent: string): Promise<void> {
    try {
      console.log(`Processing all categories in one go`);
      const allCategoriesSummaryData =
        await this.getAllCategoriesSummaryAsValidatedJSON(codeBlocksContent);
      if (!allCategoriesSummaryData) return;
      await this.appSummariesRepository.createOrReplaceAppSummary({
        projectName: this.projectName,
        llmProvider: this.llmProviderDescription,
        ...allCategoriesSummaryData,
      });
      console.log(`Captured summary details of all categories into database`);
    } catch (error: unknown) {
      logErrorMsgAndDetail(
        `Unable to generate summary data for all app categories details into database`,
        error,
      );
    }
  }

  /**
   * Generate insights as validated JSON by calling the LLM with a prompt covering all categories
   */
  private async getAllCategoriesSummaryAsValidatedJSON(
    codeBlocksContent: string,
  ): Promise<AppSummaryRecordCategories | null> {
    try {
      const instructions: readonly string[] = Object.values(summaryCategoriesConfig).flatMap(
        (category) => {
          const inst = category.instructions;
          if (Array.isArray(inst) && inst.length > 0) {
            if (typeof inst[0] === "object" && "points" in inst[0]) {
              // It's an InstructionSection array
              return inst.flatMap((section: { points: readonly string[] }) => section.points);
            }
          }
          return inst as readonly string[];
        },
      );
      const prompt = this.createInsightsAllCategoriesPrompt(instructions, codeBlocksContent);
      const llmResponse = await this.llmRouter.executeCompletion<AppSummaryRecordCategories>(
        "all-categories",
        prompt,
        {
          outputFormat: LLMOutputFormat.JSON,
          jsonSchema: appSummaryRecordCategoriesSchema,
          hasComplexSchema: ALL_CATEGORIES_SCHEMA_IS_VERTEXAI_INCOMPATIBLE,
        },
      );
      return llmResponse;
    } catch (error: unknown) {
      logWarningMsg(
        `${error instanceof Error ? error.message : "Unknown error"} for getting summary data for all categories`,
      );
      return null;
    }
  }

  /**
   * Create a prompt for the LLM to generate insights for all categories
   */
  private createInsightsAllCategoriesPrompt(
    instructions: readonly string[],
    codeBlocksContent: string,
  ): string {
    return new Prompt(
      SINGLE_PASS_INSIGHTS_TEMPLATE,
      "list of file summaries",
      instructions,
      appSummaryRecordCategoriesSchema,
      codeBlocksContent,
    ).render();
  }
}
